    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?> 
    <configuration>
        <property>
            <name>hive.execution.engine</name>
            <value>spark</value>
        </property>
        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://hive-metastore-svc:9083</value>
        </property>
        <property>
            <name>hive.metastore.schema.verification</name>
            <value>false</value> 
        </property> 
        <property>
            <name>hive.metastore.client.socket.timeout</name>
            <value>300</value>
        </property>
        <property> 
            <name>hive.metastore.warehouse.dir</name>
            <value>file:///opt/hive/warehouse</value>
            <description/> 
        </property>
        <property>
            <name>hive.metastore.event.db.notification.api.auth</name>
            <value>false</value>
            <description/>
        </property>
        <property> 
            <name>hive.exec.scratchdir</name> 
            <value>file:///opt/hive/scratchdir</value>
            <description/> 
        </property>
        <property> 
            <name>hive.exec.dynamic.partition</name> 
            <value>true</value>
            <description/> 
        </property>
        <property> 
            <name>hive.exec.dynamic.partition.mode</name> 
            <value>nonstrict</value>
            <description/> 
        </property>
        <property> 
            <name>fs.default.name</name> 
            <value>file:///</value> 
        </property>
        <property>
            <name>hive.server2.use.SSL</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.server2.keystore.path</name>
            <value>file:///opt/spark/shared/hive/server.keystore</value>
        </property>
        <property>
            <name>hive.server2.keystore.password</name>
            <value>just4esa</value>
        </property>
        <property>
            <name>hive.server2.authentication</name>
            <value>LDAP</value>
        </property>
        <property>
           <name>hive.server2.authentication.ldap.url</name>
           <value>ldaps://ldap-proxy-svc:636</value>
        </property>
        <property>
            <name>hive.server2.authentication.ldap.baseDN</name>
            <value>DC=gomel,DC=iba,DC=by</value>
        </property>
        <property>
            <name>hive.security.authorization.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.server2.enable.doAs</name>
            <value>false</value>
         </property>
        <property>
            <name>hive.users.in.admin.role</name>
            <value>maleksahin</value>
        </property>
        <property>
            <name>hive.security.metastore.authorization.manager</name>
            <value>org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly</value>
        </property>
        <property>
            <name>hive.security.authorization.manager</name>
            <value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory</value>
        </property>
        <property>
            <name>hive.security.authenticator.manager</name>
            <value>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator</value>
        </property>
        <property>
            <name>hive.security.authorization.sqlstd.confwhitelist.append</name>
            <value>param_1|param_2|param_3|param_4|param_5|param_6|param_7|param_8|param_9|param_10</value>
        </property>
        <property>
           <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:derby:;databaseName=/opt/derby/metastore_db;create=true</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>org.apache.derby.jdbc.EmbeddedDriver</value>
            <description>Driver class name for a JDBC metastore</description>
        </property>
        <property>
            <name>javax.jdo.PersistenceManagerFactoryClass</name>
            <value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
            <description>class implementing the jdo persistence</description>
        </property>
        <property>
            <name>mapreduce.input.fileinputformat.split.maxsize</name>
            <value>750000000</value>
        </property>
        <property>
            <name>hive.vectorized.execution.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.cbo.enable</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.optimize.reducededuplication.min.reducer</name>
            <value>4</value>
        </property>
        <property>
            <name>hive.optimize.reducededuplication</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.orc.splits.include.file.footer</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.merge.mapfiles</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.merge.sparkfiles</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.merge.smallfiles.avgsize</name>
            <value>16000000</value>
        </property>
        <property>
            <name>hive.merge.size.per.task</name>
            <value>256000000</value>
        </property>
        <property>
            <name>hive.merge.orcfile.stripe.level</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.auto.convert.join</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.auto.convert.join.noconditionaltask</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.auto.convert.join.noconditionaltask.size</name>
            <value>894435328</value>
        </property>
        <property>
            <name>hive.optimize.bucketmapjoin.sortedmerge</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.map.aggr.hash.percentmemory</name>
            <value>0.5</value>
        </property>
        <property>
            <name>hive.map.aggr</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.optimize.sort.dynamic.partition</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.stats.autogather</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.stats.fetch.column.stats</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.vectorized.execution.reduce.enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>hive.vectorized.groupby.checkinterval</name>
            <value>4096</value>
        </property>
        <property>
            <name>hive.vectorized.groupby.flush.percent</name>
            <value>0.1</value>
        </property>
        <property>
            <name>hive.compute.query.using.stats</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.limit.pushdown.memory.usage</name>
            <value>0.4</value>
        </property>
        <property>
            <name>hive.optimize.index.filter</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.exec.reducers.bytes.per.reducer</name>
            <value>67108864</value>
        </property>
        <property>
            <name>hive.smbjoin.cache.rows</name>
            <value>10000</value>
        </property>
        <property>
            <name>hive.exec.orc.default.stripe.size</name>
            <value>67108864</value>
        </property>
        <property>
            <name>hive.fetch.task.conversion</name>
            <value>more</value>
        </property>
        <property>
            <name>hive.fetch.task.conversion.threshold</name>
            <value>1073741824</value>
        </property>
        <property>
            <name>hive.fetch.task.aggr</name>
            <value>false</value>
        </property>
        <property>
            <name>mapreduce.input.fileinputformat.list-status.num-threads</name>
            <value>5</value>
        </property>
        <property>
            <name>spark.kryo.referenceTracking</name>
            <value>false</value>
        </property>
        <property>
            <name>spark.kryo.classesToRegister</name>
            <value>org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch</value>
        </property>
        <property>
            <name>spark.executor.cores</name>
            <value>5</value>
        </property>
        <property>
            <name>spark.executor.memory</name>
            <value>10G</value>
        </property>
        <property> 
            <name>hive.input.dir.recursive</name>
            <value>true</value> 
        </property> 
        <property> 
            <name>hive.mapred.supports.subdirectories</name>
            <value>true</value> 
        </property> 
        <property> 
            <name>hive.supports.subdirectories</name>
            <value>true</value> 
        </property> 
        <property> 
            <name>mapred.input.dir.recursive</name>
            <value>true</value> 
        </property>
        <property>
            <name>mapreduce.input.fileinputformat.input.dir.recursive</name>
            <value>true</value>
        </property>
        <property>
            <name>hive.strict.checks.cartesian.product</name>
            <value>false</value>
        </property>
    </configuration>